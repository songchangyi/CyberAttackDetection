{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GET wordbag size(1637)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "all the input arrays must have same number of dimensions",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-4a798e92a8b9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    235\u001b[0m     \u001b[0mload_wordbag\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"./xss-train.txt\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m2000\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    236\u001b[0m     \u001b[1;31m#print  wordbag.keys()\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 237\u001b[1;33m     \u001b[0mremodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"./xss-train.txt\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    238\u001b[0m     \u001b[1;31m#test_normal(remodel, sys.argv[2])\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    239\u001b[0m     \u001b[0mtest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mremodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"./xss-train.txt\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-8-4a798e92a8b9>\u001b[0m in \u001b[0;36mmain\u001b[1;34m(filename)\u001b[0m\n\u001b[0;32m    140\u001b[0m             \u001b[0mnp_vers\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvers\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    141\u001b[0m             \u001b[1;31m#print np_vers\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 142\u001b[1;33m             \u001b[0mX\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mnp_vers\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    143\u001b[0m             \u001b[0mX_lens\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp_vers\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    144\u001b[0m             \u001b[1;31m#print X_lens\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: all the input arrays must have same number of dimensions"
     ]
    }
   ],
   "source": [
    "# -*- coding:utf-8 -*-\n",
    "#!pip install hmmlearn\n",
    "import sys\n",
    "import urllib\n",
    "import urlparse\n",
    "import re\n",
    "from hmmlearn import hmm\n",
    "import numpy as np\n",
    "from sklearn.externals import joblib\n",
    "import HTMLParser\n",
    "import nltk\n",
    "import io\n",
    "\n",
    "#处理参数值的最小长度\n",
    "MIN_LEN=10\n",
    "\n",
    "#状态个数\n",
    "N=5\n",
    "#最大似然概率阈值\n",
    "T=-200\n",
    "#字母\n",
    "#数字 1\n",
    "#<>,:\"'\n",
    "#其他字符2\n",
    "SEN=['<','>',',',':','\\'','/',';','\"','{','}','(',')']\n",
    "\n",
    "index_wordbag=1 #词袋索引\n",
    "wordbag={} #词袋\n",
    "\n",
    "#</script><script>alert(String.fromCharCode(88,83,83))</script>\n",
    "#<IMG SRC=x onchange=\"alert(String.fromCharCode(88,83,83))\">\n",
    "#<;IFRAME SRC=http://ha.ckers.org/scriptlet.html <;\n",
    "#';alert(String.fromCharCode(88,83,83))//\\';alert(String.fromCharCode(88,83,83))//\";alert(String.fromCharCode(88,83,83))\n",
    "# //\\\";alert(String.fromCharCode(88,83,83))//--></SCRIPT>\">'><SCRIPT>alert(String.fromCharCode(88,83,83))</SCRIPT>\n",
    "tokens_pattern = r'''(?x)\n",
    " \"[^\"]+\"\n",
    "|http://\\S+\n",
    "|</\\w+>\n",
    "|<\\w+>\n",
    "|<\\w+\n",
    "|\\w+=\n",
    "|>\n",
    "|\\w+\\([^<]+\\) #函数 比如alert(String.fromCharCode(88,83,83))\n",
    "|\\w+\n",
    "'''\n",
    "\n",
    "def ischeck(str):\n",
    "    if re.match(r'^(http)',str):\n",
    "        return False\n",
    "    for i, c in enumerate(str):\n",
    "        if ord(c) > 127 or ord(c) < 31:\n",
    "            return False\n",
    "        if c in SEN:\n",
    "            return True\n",
    "        #排除中文干扰 只处理127以内的字符\n",
    "\n",
    "\n",
    "    return False\n",
    "\n",
    "\n",
    "def do_str(line):\n",
    "    words=nltk.regexp_tokenize(line, tokens_pattern)\n",
    "    #print  words\n",
    "    return words\n",
    "\n",
    "def load_wordbag(filename,max=100):\n",
    "    X = [[0]]\n",
    "    X_lens = [1]\n",
    "    tokens_list=[]\n",
    "    global wordbag\n",
    "    global index_wordbag\n",
    "\n",
    "    with io.open(filename,encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            line=line.strip('\\n')\n",
    "            #url解码\n",
    "            line=urllib.unquote(line)\n",
    "            #处理html转义字符\n",
    "            h = HTMLParser.HTMLParser()\n",
    "            line=h.unescape(line)\n",
    "            if len(line) >= MIN_LEN:\n",
    "                #print \"Learning xss query param:(%s)\" % line\n",
    "                #数字常量替换成8\n",
    "                line, number = re.subn(r'\\d+', \"8\", line)\n",
    "                #ulr日换成http://u\n",
    "                line, number = re.subn(r'(http|https)://[a-zA-Z0-9\\.@&/#!#\\?:=]+', \"http://u\", line)\n",
    "                #干掉注释\n",
    "                line, number = re.subn(r'\\/\\*.?\\*\\/', \"\", line)\n",
    "                #print \"Learning xss query etl param:(%s) \" % line\n",
    "                tokens_list+=do_str(line)\n",
    "\n",
    "            #X=np.concatenate( [X,vers])\n",
    "            #X_lens.append(len(vers))\n",
    "\n",
    "\n",
    "    fredist = nltk.FreqDist(tokens_list)  # 单文件词频\n",
    "    keys=fredist.keys()\n",
    "    keys=keys[:max]\n",
    "    for localkey in keys:  # 获取统计后的不重复词集\n",
    "        if localkey in wordbag.keys():  # 判断该词是否已在词袋中\n",
    "            continue\n",
    "        else:\n",
    "            wordbag[localkey] = index_wordbag\n",
    "            index_wordbag += 1\n",
    "\n",
    "    print \"GET wordbag size(%d)\" % index_wordbag\n",
    "def main(filename):\n",
    "    X = [[-1]]\n",
    "    X_lens = [1]\n",
    "    global wordbag\n",
    "    global index_wordbag\n",
    "\n",
    "    with io.open(filename,encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            line=line.strip('\\n')\n",
    "            #url解码\n",
    "            line=urllib.unquote(line)\n",
    "            #处理html转义字符\n",
    "            h = HTMLParser.HTMLParser()\n",
    "            line=h.unescape(line)\n",
    "            vers=[]\n",
    "            if len(line) >= MIN_LEN:\n",
    "                #print \"Learning xss query param:(%s)\" % line\n",
    "                #数字常量替换成8\n",
    "                line, number = re.subn(r'\\d+', \"8\", line)\n",
    "                #ulr日换成http://u\n",
    "                line, number = re.subn(r'(http|https)://[a-zA-Z0-9\\.@&/#!#\\?:]+', \"http://u\", line)\n",
    "                #干掉注释\n",
    "                line, number = re.subn(r'\\/\\*.?\\*\\/', \"\", line)\n",
    "                #print \"Learning xss query etl param:(%s) \" % line\n",
    "                words=do_str(line)\n",
    "                vers=[]\n",
    "                for word in words:\n",
    "                    #print \"ADD %s\" % word\n",
    "                    if word in wordbag.keys():\n",
    "                        vers.append([wordbag[word]])\n",
    "                    else:\n",
    "                        vers.append([-1])\n",
    "\n",
    "            np_vers = np.array(vers)\n",
    "            #print np_vers\n",
    "            X=np.concatenate([X,np_vers])\n",
    "            X_lens.append(len(np_vers))\n",
    "            #print X_lens\n",
    "\n",
    "\n",
    "\n",
    "    remodel = hmm.GaussianHMM(n_components=N, covariance_type=\"full\", n_iter=100)\n",
    "    remodel.fit(X,X_lens)\n",
    "    joblib.dump(remodel, \"xss-train.pkl\")\n",
    "\n",
    "    return remodel\n",
    "\n",
    "def test(remodel,filename):\n",
    "    with io.open(filename,encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            line = line.strip('\\n')\n",
    "            # url解码\n",
    "            line = urllib.unquote(line)\n",
    "            # 处理html转义字符\n",
    "            h = HTMLParser.HTMLParser()\n",
    "            line = h.unescape(line)\n",
    "\n",
    "            if len(line) >= MIN_LEN:\n",
    "                #print  \"CHK XSS_URL:(%s) \" % (line)\n",
    "                    # 数字常量替换成8\n",
    "                line, number = re.subn(r'\\d+', \"8\", line)\n",
    "                    # ulr日换成http://u\n",
    "                line, number = re.subn(r'(http|https)://[a-zA-Z0-9\\.@&/#!#\\?:]+', \"http://u\", line)\n",
    "                    # 干掉注释\n",
    "                line, number = re.subn(r'\\/\\*.?\\*\\/', \"\", line)\n",
    "                    # print \"Learning xss query etl param:(%s) \" % line\n",
    "                words = do_str(line)\n",
    "                #print \"GET Tokens (%s)\" % words\n",
    "                vers = []\n",
    "                for word in words:\n",
    "                        # print \"ADD %s\" % word\n",
    "                    if word in wordbag.keys():\n",
    "                        vers.append([wordbag[word]])\n",
    "                    else:\n",
    "                        vers.append([-1])\n",
    "\n",
    "                np_vers = np.array(vers)\n",
    "                #print np_vers\n",
    "                        #print  \"CHK SCORE:(%d) QUREY_PARAM:(%s) XSS_URL:(%s) \" % (pro, v, line)\n",
    "                pro = remodel.score(np_vers)\n",
    "\n",
    "                if pro >= T:\n",
    "                    print  \"SCORE:(%d) XSS_URL:(%s) \" % (pro,line)\n",
    "                        #print line\n",
    "\n",
    "def test_normal(remodel,filename):\n",
    "    with io.open(filename,encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            # 切割参数\n",
    "            result = urlparse.urlparse(line)\n",
    "            # url解码\n",
    "            query = urllib.unquote(result.query)\n",
    "            params = urlparse.parse_qsl(query, True)\n",
    "\n",
    "            for k, v in params:\n",
    "                v=v.strip('\\n')\n",
    "                #print  \"CHECK v:%s LINE:%s \" % (v, line)\n",
    "\n",
    "                if len(v) >= MIN_LEN:\n",
    "                    # print  \"CHK XSS_URL:(%s) \" % (line)\n",
    "                    # 数字常量替换成8\n",
    "                    v, number = re.subn(r'\\d+', \"8\", v)\n",
    "                    # ulr日换成http://u\n",
    "                    v, number = re.subn(r'(http|https)://[a-zA-Z0-9\\.@&/#!#\\?:]+', \"http://u\", v)\n",
    "                    # 干掉注释\n",
    "                    v, number = re.subn(r'\\/\\*.?\\*\\/', \"\", v)\n",
    "                    # print \"Learning xss query etl param:(%s) \" % line\n",
    "                    words = do_str(v)\n",
    "                    # print \"GET Tokens (%s)\" % words\n",
    "                    vers = []\n",
    "                    for word in words:\n",
    "                        # print \"ADD %s\" % word\n",
    "                        if word in wordbag.keys():\n",
    "                            vers.append([wordbag[word]])\n",
    "                        else:\n",
    "                            vers.append([-1])\n",
    "\n",
    "                    np_vers = np.array(vers)\n",
    "                    # print np_vers\n",
    "                    # print  \"CHK SCORE:(%d) QUREY_PARAM:(%s) XSS_URL:(%s) \" % (pro, v, line)\n",
    "                    pro = remodel.score(np_vers)\n",
    "                    print  \"CHK SCORE:(%d) QUREY_PARAM:(%s)\" % (pro, v)\n",
    "                    #if pro >= T:\n",
    "                        #print  \"SCORE:(%d) XSS_URL:(%s) \" % (pro, v)\n",
    "                        #print line\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    #test(remodel,sys.argv[2])\n",
    "    load_wordbag(\"./xss-train.txt\",2000)\n",
    "    #print  wordbag.keys()\n",
    "    remodel = main(\"./xss-train.txt\")\n",
    "    #test_normal(remodel, sys.argv[2])\n",
    "    test(remodel, \"./xss-train.txt\")"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
