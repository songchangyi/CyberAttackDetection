{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import ES & Libs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pandas import Series,DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from elasticsearch import Elasticsearch\n",
    "es = Elasticsearch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get exist data\n",
    "res = es.search(index=\"people\", body={\"query\": {\"match_all\": {}}})\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Index data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import file\n",
    "BF_logs_raw = pd.read_csv(\"./data/BF_datalab_httpd_all.log\", header = None, delimiter = \" \")\n",
    "log_length = BF_logs_raw.shape[0]\n",
    "BF_logs_raw.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from elasticsearch import helpers\n",
    "\n",
    "def bulkData(length, log_type):\n",
    "    es = Elasticsearch()\n",
    "    j = 0\n",
    "    actions = []\n",
    "    while (j < length):\n",
    "        log = BF_logs_raw.loc[j]\n",
    "        action = {\n",
    "            \"_index\": 'log',\n",
    "            \"_type\": log_type,\n",
    "            \"_id\": j,\n",
    "            \"_source\": {\n",
    "                \"timestamp\":log[3],\n",
    "                \"action\":log[5],\n",
    "                \"status\":log[6],\n",
    "                \"respsize\":log[7],\n",
    "                \"data\":log[9],\n",
    "                \"timed\":log[10]\n",
    "            }\n",
    "        }\n",
    "        actions.append(action)\n",
    "        j += 1\n",
    "    # use bulk to POST data\n",
    "    if (len(actions) > 0):\n",
    "        helpers.bulk(es, actions)\n",
    "        del actions[0:len(actions)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# execute function to store data\n",
    "bulkData(log_length, 'bf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For other logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normal_logs_raw = pd.read_csv(\"./data/normal.log\", header = None, delimiter = \" \")\n",
    "sql_logs_raw = pd.read_csv(\"./data/sql.log\", header = None, delimiter = \" \")\n",
    "xss_logs_raw = pd.read_csv(\"./data/xss.log\", header = None, delimiter = \" \",error_bad_lines=False)\n",
    "\n",
    "log_length_normal = normal_logs_raw.shape[0]\n",
    "log_length_sql = sql_logs_raw.shape[0]\n",
    "log_length_xss = xss_logs_raw.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def bulkData(length, log_type):\n",
    "    es = Elasticsearch()\n",
    "    j = 0\n",
    "    actions = []\n",
    "    # give diffrent separators to diffrent logs\n",
    "    if log_type == 'bf':\n",
    "        separator = 0\n",
    "    if log_type == 'normal':\n",
    "        separator = 200000\n",
    "    if log_type == 'sql':\n",
    "        separator = 400000\n",
    "    if log_type == 'xss':\n",
    "        separator = 600000\n",
    "    while (j < length):\n",
    "        log = BF_logs_raw.loc[j]\n",
    "        action = {\n",
    "            \"_index\": 'log',\n",
    "            \"_type\": log_type,\n",
    "            \"_id\": j + separator,\n",
    "            \"_source\": {\n",
    "                \"timestamp\":log[3],\n",
    "                \"action\":log[5],\n",
    "                \"status\":log[6],\n",
    "                \"respsize\":log[7],\n",
    "                \"data\":log[9],\n",
    "                \"timed\":log[10]\n",
    "            }\n",
    "        }\n",
    "        actions.append(action)\n",
    "        j += 1\n",
    "    # use bulk to POST data\n",
    "    if (len(actions) > 0):\n",
    "        helpers.bulk(es, actions)\n",
    "        del actions[0:len(actions)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bulkData(log_length_normal, 'normal')\n",
    "bulkData(log_length_sql, 'sql')\n",
    "bulkData(log_length_xss, 'xss')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "es.search(index=\"log\", doc_type='normal',body={\"query\": {\"match_all\": {}}})"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
